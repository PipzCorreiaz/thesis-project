\subsection{\gls{ai} in games}
 
%%% jogos tem sido um grande desafio
There are many games that \gls{ai} has been solving over the years.
However, the definition of "games" usually refers to zero-sum and perfect information games.
This kind of games is commonly solved by creating a tree search representing all the possible states and searching for the optimal or a nearly optimal solution.
The greatest achievements are generally based on finding good heuristics to refine the search and also good prunings to reduce the space of search.
Deepblue can exemplify this idea \cite{Campbell2002}.
It uses an iterative-deepening alpha-beta search and the key of its success is mostly the null move heuristic and the futility pruning.
Another example is Chinook, also a perfect information game that was solved using alpha-beta search \cite{Schaeffer1996}.

%%% perfect e imperfect nao se resolvem da mesma maneira
Nevertheless, \emph{Sueca} is considered an imperfect information game, as described in section \ref{sec:background}.
This class of games is usually solved with three different approaches \cite{Cowling2012}.
The first one and the most popular is based on Monte Carlo Methods.
Secondly, a possible approach is trying to compute a Nash equilibrium strategy or an approximation.
Lastly, there is the usage of belief distributions involving game state inferences and opponent models.
The first two mentioned approaches are mutually exclusive.
However, the last one can be used simultaneously as a supplement.
The next two subsections will describe in more detail how Monte Carlo Methods and Belief Distributions can be addressed in hidden information games.
The second pointed approach will not be addressed due to the imposed limitations of our domain.
For instance, the maximum known number of states for computing a Nash equilibrium is $10^{12}$, which is much lower than the number of possible states in a \emph{Sueca} game.




\subsubsection{Monte Carlo Methods}

%%% monte carlo e cada vez mais popular
The popularity and acceptance of Monte Carlo based Methods has increased since its success on Bridge.
\gls{gib}\footnote{http://www.gibware.com/} was the first computer bridge champion using Monte Carlo Methods.
Subsequently, another two successful domains were Skat\footnote{https://skatgame.net/} and Computer Go \cite{Gelly2011}.
Regarding that some of these domains remain a challenge for traditional \gls{ai} techniques, this method seems to be very promising.


%%% diferenca entre MCTS e PIMC
In order to solve a hidden information game, the first challenge is to deal with the information sets.
The most used approach is determinization, which samples choice nodes instead of considering all of them in an unique set.
Applying this approach to \gls{mcts} is known as \gls{pimc}.
For instance in a card game scenario, each iteration of the algorithm samples the cards distributions for all players and the simulation process of the game behaves as a perfect information game.
In other words, during the simulation each player makes decisions as if his opponents' cards are visible.
The first successful implementation of this technique was \gls{gib} \cite{Ginsberg2001}.


%%% Basin estudou os prblomeas do PIMC
In 1998, Frank and Basin produced an analysis on \gls{pimc}'s limitations \cite{Frank1998}.
They identified two distinct problems: \emph{strategy fusion} and \emph{non-locality}.
Since \gls{pimc} has a repeated minimaxing architecture, it evaluates the possible distributions with the best strategy.
As a result, applying this knowledge when information is missing might produce suboptimal decisions and it is called the \emph{strategy fusion}.
For instance, when having a move with a guaranteed reward and another move with a possible reward of the same value although depending on the current world, \gls{pimc} equally considers both moves.
The second problem, \emph{non-locality}, results from the propagation of values.
The value of a game tree node only considers his children' values.
However, in an imperfect information game, some guesses might be done considering values of the non-local subtree.
For instance, considering 2 different worlds, the player 1 can guarantee a winning trick in the world 1 by making a certain move.
If in that state he makes another move, player 2 might assume they are in world 2.
\gls{pimc} cannot make such an inference.


%%% O Long estudou com que propriedades o PICM funciona
Despite the fortunate outcomes of \gls{pimc}, there still were difficulties in understanding the strong results of this algorithm, specially considering the previously mentioned problems.
As a result, Long et. al. have analysed carefully the expected \gls{pimc}' errors mentioned by critics \cite{Long2010}.
These mistakes lead him to find three different properties of a game and test its influences on the success of \gls{pimc}.
The first property is \emph{leaf correlation}, which demonstrates how likely it is to affect a player's payoff in the neighbourhood of a leaf.
The probability of all siblings having the same payoff values is higher as the correlation increases.
Secondly, \emph{bias} indicates the chance of a player being preferred over the other.
Finally, the last characteristic of a game that has been pointed is \emph{disambiguation factor}, that denotes how rapidly the hidden information is revealed.
These properties have been tested in a set of experiments in both \gls{pimc} and a random player against an optimal Nash-equilibrium player.
The performance of \gls{pimc} increases as the correlation value is higher.
It has also been shown that bias does not considerably affect its success.
Finally, disambiguation has the greatest impact on the results of the algorithm.
When this last value is higher, it means the game turns more quickly into a perfect information game.
Additionally, Buro demonstrates real game examples of theses properties, for Skat and Kuhn poker.
Due to its properties configurations, Skat indicates a considerably good performance of \gls{pimc}.
Since Skat presents strong similarities to \emph{Sueca}, it is expected that \gls{pimc} also has a good performance when applied to \emph{Sueca}.


%%%  cowling propose ISMCTS
Cowling et. al. have also investigated the application of \gls{mcts} to hidden information games \cite{Cowling2012}.
Their research supports a new descendant family of algorithms, \gls{ismcts}.
\gls{ismcts} works with information sets, instead of game states.
It also uses determinization to sample the game, however produces a single tree.
The main advantages are the computational space efficiency and fact of suffering less \emph{strategy fusion} than \gls{pimc}.
The authors presented some experiments in three different games, including a card game.
Their results on the card game Dou Di Zhu were very similar to \gls{uct}.
This evidence has discouraged the usage of this technique on the domain of \emph{Sueca}.


%%% IIMC
Recently, Furtak and Buro \cite{Furtak} presented a new search algorithm called \gls{iimc}.
It can be suitably applied to hidden information games and reduces the strategy fusion problem.
During the simulation phase, each player's move is chosen inside a player's module.
The game behaves as an imperfect information due to this encapsulation.
Additionally, the players' modules allow the differentiation of players using different strategies.
The authors revealed the great potential of this approach when applied to trick-based card games, since it has been tested in the Skat scenario.

Due to the similarity between \emph{Sueca} and Skat, it is predictable that Monte Carlo methods might produce good results.
However, assuming the exact same behaviour when applying different variations of \gls{pimc} might be a mistake, considering certain domain characteristics.
As a result, using the \gls{iimc} approach before trying \gls{pimc} could not be a reasonable strategy.



\subsubsection{Game State Inference \& Opponent Modelling}


While discussing imperfect information games, it is relevant to mention inference of information.
Predicting some of the opponents' cards or other clues would be valuable to select better actions in each state of the game.
This problem is known as finding the $P$(world\textbar move) for a move played in an hypothetical world.


Buro in 2009 \cite{Buro} presented his work on state evaluation and inference that has been included in his Skat player.
His approach combines two techniques, one for evaluating the bidding and another for selecting hypothetical worlds during the game play.
The former technique uses a logistic regression to evaluate the winning probability of each hand.
In order to do this evaluation, 22 million Skat games were used as data base.
This winning probability determines the strength of a hand and therefore can be used on the bidding.
The second technique is mainly based on two heuristics.
Fastest-cut-first search heuristic evaluates each move according to its beta-cutoff and sorts all the moves.
Additionally, another heuristic is used to reduce the tree exploration.
It groups cards by their strength value and considers, for example, 7\ding{168} and 8\ding{168} the same move, when holding both cards in a player's hand.
The author compares his work to other similar ones and concludes the strength of his techniques lies at two central points.
First, determining the $P$(world\textbar move) on offline data, instead of doing it in runtime.
Lastly, his formulation is generalised in a way that it is possible to perform it on high-level features.
The main difference between \emph{Sueca} and Skat is that the first one does not have the bidding phase.
Due to this discrepancy, Buro's first technique would not be appropriate for the \emph{Sueca} game.
However, the search enhancements could be suitably applied since the game trees are identical.


Usually, opponent modelling uses optimal strategies to predict other players' actions.
Therefore, these models tend to be overly defensive.
Consequently, Long and Buro in 2011 \cite{Long2009} suggests a post-processing analysis that is able to infer opponent's qualities based on their decisions in a certain environment.
The main idea is to classify each opponent with a mistake rate and use that value to be more or less defencive.
This approach, called \gls{pipma}, computes a procedure after each episode of the game (in a trick-taking card-game, it would be after each trick) to update incrementally the mistake rate of each opponent.
The authors made some experiments in a Skat player with very good results.
The mistake rate adjusted the bidding behaviour during the game.
Likewise, the search improvements reduced about 40\% of the search space.
Despite the fact that \emph{Sueca} does not have the bidding phase, playing some cards under a given hand might be considered more or less aggressive.
As a result, it would be interesting to model the opponents in a similar way to make better decisions.

Another card game highly suitable to make opponent models is Poker.
Predicting the players' moves can naturally affect the outcome of this game.
In order to predict players' cards and their future actions, Posen et al. in 2010 \cite{Ponsen2008} reveal some research in this subject.
Their opponent model starts with a prior distribution and changes over time with a differentiating function.
The prior distribution allows it to make reasonable predictions while having insufficient information.
In addition, the relational probability tree algorithm TILDE builds a decision tree with the stored samples of a player.
This decision tree represents the differentiating function that will adapt the initial prior distribution.
Besides modelling the opponents, the authors present how to integrate this function with \gls{mcts}.
Instead of sampling the cards randomly, \gls{mcts} uses the card predictions.
Therefore, the algorithm does not need a numerous amount of iterations to reach a uniform card distribution.
Furthermore, the probabilities of action predictions are used in the selection phase of the \gls{mcts}, according to the state of the game and the sampled cards.
Since \gls{mcts} is a possible choice to solve \emph{Sueca}, a similar opponent model can also improve the capabilities of this algorithm, as shown in Poker.

 