\subsection{Monte Carlo Tree Search}


\gls{mcts} is a family of algorithms with the goal of finding optimal decisions.
This method incrementally builds a search tree according to the results of previous iterations.
The search tree is expanded by randomly sampling the nodes.
Usually, it is divided into four steps, as described below.
\begin{itemize}
  \item Selection: Select a child node through a selection policy. This policy must balance between unexplored areas of the tree and promising nodes that may lead to higher rewards.
  \item Expansion: Expand the selected node to add one or more nodes to the tree, according to the available actions.
  \item Simulation: Select an expanded node through a simulation or default policy to produce an outcome.
  \item Backpropagation: Propagate the reward value of all the selected nodes in order to update their statistics.
\end{itemize}

According to Browne et al. \cite{Browne2012}, finding a suitable variation of \gls{mcts} is the greatest challenge of applying the algorithm to a specific environment.
The most popular algorithm of \gls{mcts} family is \gls{uct} algorithm.
The \gls{uct} variation differs from the original in the selection phase.
It uses a maximisation function to evaluate the available nodes, according to the following equation:

\begin{equation}
    UCT = \overline{X}_j + 2C_p\sqrt{\frac{2\ln n}{n_j}}
\end{equation}

\todo{Review text}

It models the reward of each child node as an independent multiarmed bandit problem.
This means that besides considering the already obtained average reward of a child node, it also considers the maximum expected gain of that child node.
This function establishes an equilibrium between exploitation and exploration.
Exploitation is evaluated in the first term of the equation by considering the average reward of a child node.
The exploration is manipulated in the second term.
A considerable amount of iterations will approximate the \gls{uct} to a minimax tree.
Consequently, the produced results are nearly optimal.

 